{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057e7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08fc9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/04/25 13:22:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLAnalysisAdClicks\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 获取 SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9964112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+----------+-----+------+----------+-------+----+\n",
      "|numClick|region|city|adExchange|width|height|floorPrice|weekday|hour|\n",
      "+--------+------+----+----------+-----+------+----------+-------+----+\n",
      "|       0|    94| 100|         2|  468|    60|        13| Monday|   0|\n",
      "|       0|    40|  42|         2|  728|    90|         5| Monday|   0|\n",
      "|       0|    40|  45|         1|  160|   600|         0| Monday|   0|\n",
      "|       0|    80|  85|         2|  300|   250|         5| Monday|   0|\n",
      "|       0|     0|   0|         2|  300|   250|         5| Monday|   0|\n",
      "+--------+------+----+----------+-----+------+----------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load normalized data\n",
    "df = spark.read.option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/bw2298/combined_training_before_normalize.txt\")\n",
    "\n",
    "# Select necessary columns and cast them as needed\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.select(\n",
    "    col(\"numClick\").cast(\"int\"),\n",
    "    col(\"region\").cast(\"int\"),\n",
    "    col(\"city\").cast(\"int\"),\n",
    "    col(\"adExchange\").cast(\"int\"),\n",
    "    col(\"width\").cast(\"int\"),\n",
    "    col(\"height\").cast(\"int\"),\n",
    "    col(\"floorPrice\").cast(\"int\"),\n",
    "    col(\"weekday\"),\n",
    "    col(\"hour\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# Check the data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67cb44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# Indexing and encoding categorical columns\n",
    "categoricalColumns = ['region', 'city', 'adExchange', 'weekday']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Assembling vector\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + ['width', 'height', 'floorPrice', 'hour']\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132228ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 13:23:26 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/04/25 13:23:26 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC:  0.6114016201900049\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Set up the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"numClick\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"numClick\", featuresCol=\"features\", maxIter=10)\n",
    "pipeline_lr = Pipeline(stages=stages + [lr])\n",
    "model_lr = pipeline_lr.fit(df)\n",
    "predictions_lr = model_lr.transform(df)\n",
    "print(\"Logistic Regression AUC: \", evaluator.evaluate(predictions_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2221d082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 162:====================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting AUC:  0.636350438444325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Trees\n",
    "gbt = GBTClassifier(labelCol=\"numClick\", featuresCol=\"features\", maxIter=10)\n",
    "pipeline_gbt = Pipeline(stages=stages + [gbt])\n",
    "model_gbt = pipeline_gbt.fit(df)\n",
    "predictions_gbt = model_gbt.transform(df)\n",
    "print(\"Gradient Boosting AUC: \", evaluator.evaluate(predictions_gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8145dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM AUC:  0.51549962635207\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "svm = LinearSVC(labelCol=\"numClick\", featuresCol=\"features\", maxIter=10)\n",
    "pipeline_svm = Pipeline(stages=stages + [svm])\n",
    "model_svm = pipeline_svm.fit(df)\n",
    "predictions_svm = model_svm.transform(df)\n",
    "print(\"SVM AUC: \", evaluator.evaluate(predictions_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf49e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
