{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normalized data\n",
    "df = spark.read.option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/yl9709/combined_training_before_normalize.txt\")\n",
    "\n",
    "# Select necessary columns and cast them as needed\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.select(\n",
    "    col(\"numClick\").cast(\"int\"),\n",
    "    col(\"region\").cast(\"int\"),\n",
    "    col(\"city\").cast(\"int\"),\n",
    "    col(\"adExchange\").cast(\"int\"),\n",
    "    col(\"width\").cast(\"int\"),\n",
    "    col(\"height\").cast(\"int\"),\n",
    "    col(\"floorPrice\").cast(\"int\"),\n",
    "    col(\"weekday\"),\n",
    "    col(\"hour\").cast(\"int\"),\n",
    "    col(\"payingPrice\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# Check the data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Adding a unique ID to test_df before splitting into train/test or applying transformations\n",
    "df = df.withColumn(\"unique_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# Indexing and encoding categorical columns\n",
    "categoricalColumns = ['region', 'city', 'adExchange', 'weekday']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Assembling vector\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + ['width', 'height', 'floorPrice', 'hour']\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Set up the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"numClick\", metricName=\"areaUnderROC\")\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"numClick\", featuresCol=\"features\", maxIter=10)\n",
    "pipeline_lr = Pipeline(stages=stages + [lr])\n",
    "model_lr = pipeline_lr.fit(train_df)\n",
    "predictions_lr = model_lr.transform(test_df)\n",
    "print(\"Logistic Regression AUC: \", evaluator.evaluate(predictions_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Trees\n",
    "gbt = GBTClassifier(labelCol=\"numClick\", featuresCol=\"features\", maxIter=10)\n",
    "pipeline_gbt = Pipeline(stages=stages + [gbt])\n",
    "model_gbt = pipeline_gbt.fit(train_df)\n",
    "predictions_gbt = model_gbt.transform(test_df)\n",
    "print(\"Gradient Boosting AUC: \", evaluator.evaluate(predictions_gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm = LinearSVC(labelCol=\"numClick\", featuresCol=\"features\", maxIter=10)\n",
    "pipeline_svm = Pipeline(stages=stages + [svm])\n",
    "model_svm = pipeline_svm.fit(train_df)\n",
    "predictions_svm = model_svm.transform(test_df)\n",
    "print(\"SVM AUC: \", evaluator.evaluate(predictions_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the GBT prediction column to the test data\n",
    "predictions_gbt = predictions_gbt.withColumn(\"unique_id\", monotonically_increasing_id())\n",
    "test_df= test_df.join(predictions_gbt.select('prediction', 'features'), on='features', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum bidding price to determine the budget. --- should it be paying price or bidding price\n",
    "# The max bidding in the simulation will be less than this.\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "total_paying_price = test_df.agg(sum(\"payingPrice\").alias(\"total_paying_price\")).collect()[0][\"total_paying_price\"]\n",
    "print(\"Total Paying Price: \", total_paying_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (avg winning price / average click through rate) to determine the range of the alpha\n",
    "\n",
    "from pyspark.sql.functions import avg, max, min\n",
    "results = test_df.agg(\n",
    "    avg(\"payingPrice\").alias(\"avg_paying_price\"),\n",
    "    avg(\"clickRate\").alias(\"avg_click_rate\"),\n",
    "    max(\"payingPrice\").alias(\"max_paying_price\"),\n",
    ").collect()[0]\n",
    "\n",
    "ratio_avg = results[\"avg_paying_price\"] / results[\"avg_click_rate\"]\n",
    "\n",
    "print(\"Ratio of average payingPrice to average clickRate:\", ratio_avg)\n",
    "print(\"Ratio of max payingPrice to min clickRate:\", results[\"max_paying_price\"])\n",
    "\n",
    "# bid price = click through rate * alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Bidding Function:\n",
    "(1) Bidding Price = α × Predicted Click Rate\n",
    "<br>\n",
    "<br>(2) Implement Binary Search: Use binary search to find the optimal α while staying in the budget\n",
    "<br>\n",
    "<br>(3) Simulation Function: Create a function to simulate bidding based on a given α and calculate the total clicks won and the total cost.\n",
    "<br>\n",
    "<br>(4) Optimization Loop: Use the binary search to adjust α to maximize clicks while staying under the budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate bidding to based on a given alpha - returns the total clicks and total costs\n",
    "def simulateBidding(alpha, test_df, budget):\n",
    "    total_spent = 0\n",
    "    total_clicks = 0\n",
    "    roi_multiplier = 1\n",
    "\n",
    "    for row in test_df.collect():\n",
    "        predicted_click_rate = row.prediction\n",
    "        winning_price = row.payingPrice / 1000.0\n",
    "        bidding_hour = row.hour\n",
    "        \n",
    "\n",
    "        bid_price = alpha * (predicted_click_rate * roi_dict[bidding_hour])###\n",
    "        if bid_price > winning_price:\n",
    "            total_spent += winning_price\n",
    "            total_clicks =  total_clicks + row.numClick\n",
    "\n",
    "\n",
    "    return total_clicks, total_spent\n",
    "\n",
    "\n",
    "# Use binary search to find the alpha that maximizes the click rate while staying under the budget\n",
    "def binarySearch(low, high, test_df, budget):\n",
    "    # returns the alpha\n",
    "    alpha = 0\n",
    "    max_clicks = 0\n",
    "\n",
    "    while high - low > 0.01:\n",
    "        mid = (low + high) / 2\n",
    "        clicks, spend = simulateBidding(mid, test_df, budget)\n",
    "        if spend > budget:\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid\n",
    "            if clicks > max_clicks:\n",
    "                max_clicks = clicks\n",
    "                alpha = mid\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = binarySearch(0, 100, test_df, 50980984/2) # the low & high & budget are to be changed later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour, ROI\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
